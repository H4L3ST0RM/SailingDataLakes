{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1176111b-7aa7-4b64-ada4-372891c8b88d",
   "metadata": {},
   "source": [
    "+++\n",
    "authors = [\"John C Hale\"]\n",
    "title = \"Regularized Linear Regression\"\n",
    "date = \"2024-03-12\"\n",
    "description = \"n overview of how regularized linear regression works\"\n",
    "math = true\n",
    "draft = false\n",
    "tags = [\n",
    "\"ml\",\n",
    "\"data science\",\n",
    "\"machine learning\",\n",
    "\"regression\",\n",
    "]\n",
    "categories = [\n",
    "\"Machine Learning Walkthrough\",\n",
    "]\n",
    "series = [\"ML Walkthrough\"]\n",
    "+++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae69429-9f8b-4db7-8be9-45107d01f14a",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "The goal of this article is first to develop an understanding of overfitting \n",
    "and regularization. After which, we will discuss the intuition, math, and code \n",
    "of the two primary methods of regularizing linear regression; ridge regression, \n",
    "and LASSO regression.\n",
    "\n",
    "Below were defining each of our classes. We are going to keep them barebones for this exercise, so we can highlight the important parts. Notice that the `predict()` function is the exact same for each class. That is because the underlying model is the same for all of these variations. What is different, is how we define the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2c614-4dbf-487d-a5d7-60919ad7a204",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "Overfitting usually occurs when a model is overly complex for a given problem \n",
    "or given dataset, and thus able to memorize the training set. This leads to \n",
    "excellent performance in training, but less so when testing. This is also known as having high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7d530-6114-49b9-83b7-2aca45deb69b",
   "metadata": {},
   "source": [
    "## Regular Linear Regression Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7387b197-2b47-47f9-886f-f94c1641238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(suppress=True, precision=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35e178-997b-4249-a549-e3c167682283",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The `fit()` function is where we optimize our parameters, $\\beta$. \n",
    "\n",
    "Our loss function for regular linear regression looks like this:\n",
    "$$\\vec{\\hat{\\beta}}=\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}} \\sum_{i=1}^{n}{(\\hat{\\beta} .\\vec{x_i} - y_i)^2}$$\n",
    "\n",
    "For basic linear regression below, we have a closed form solution, and we can find our optimal $\\beta$  to minimize our loss function as follows:\n",
    "$$\\hat{\\beta} = (\\vec{X}^{T} \\vec{X})^{-1} \\vec{X}^{T} \\vec{y}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254dc7e4-d8f2-4450-9236-39512eff9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression:\n",
    "    def __init__(self, X, y):\n",
    "        self.betas = self.fit(X, y)\n",
    "\n",
    "    def fit(self,X, y):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        betas = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "        return betas\n",
    "\n",
    "    def predict(self, X):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        return X.T @ self.betas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b12efb-78a3-4f81-93ff-4d4170cc94f4",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Regularization encompasses any method that acts to “punish” complexity within a\n",
    "model in an effort to prevent overfitting. This can involve adding penalties for \n",
    "heavily weighted model, but there are other methods we will discuss in a future\n",
    "article.\n",
    "\n",
    "Two common tools in regularization of machine learning algorithms are the L1-Norm\n",
    "and L2-Norm. The L1-Norm equates to Manhattan distance, and the L2-Norm is \n",
    "equivalent to Euclidean distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a14114-a3a8-4e59-b9b4-9cca91989953",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Ridge regularization (L2-Regularization) effectively works by taking the square\n",
    "of the coefficients, summing them together, and then taking the square\n",
    "root. The calculation is appended to the cost function discussed in the linear \n",
    "regression post. Doing this effectively punishes coefficients for having large \n",
    "magnitudes. It pushes the coefficents *towards* zero.\n",
    "\n",
    "works as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f15281-8f90-441a-a788-7241579e9c47",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\vec{\\hat{\\beta}}=\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}} \\sum_{i=1}^{n}{(\\hat{\\beta} .x_i - y_i)^2}+\\lambda\\hat{\\beta_i}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(D,\\vec{\\beta})=||X\\vec{\\beta} - Y||^2 + \\lambda \\bf{I} ||\\vec{\\beta}||^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "=(X\\vec{\\beta}-y)^T(X\\vec{\\beta}-Y)+\\lambda \\bf{I} \\vec{\\beta}^T \\vec{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}^TX^TX\\vec{\\beta} + \\lambda \\bf{I} \\vec{\\beta}^T \\vec{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb3073-a9fb-4f25-94e7-116da44bab30",
   "metadata": {},
   "source": [
    "Get gradient w.r.t. $\\vec{\\beta}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L(D,\\vec{\\beta})}}{\\partial{\\vec{\\beta}}} = \\frac{\\partial{(Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}^TX^TX\\vec{\\beta}+\\lambda \\bf{I} \\vec{\\beta}^T \\vec{\\beta}})}{\\partial{\\vec{\\beta}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -2Y^TX+2\\vec{\\beta}^TX^TX + 2 \\lambda \\bf{I} \\vec{\\beta}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9f0ae-7946-494f-905e-7818056d12db",
   "metadata": {},
   "source": [
    "Set gradient to zero\n",
    "\n",
    "$$\n",
    "=-2Y^TX+2\\vec{\\beta}^TX^TX + \\lambda \\bf{I} \\vec{\\beta}^T=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y^TX=\\vec{\\beta}^TX^TX + \\lambda \\bf{I} \\vec{\\beta}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y^TX=\\vec{\\beta}^T(X^TX + \\lambda \\bf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^TY=(X^TX + \\lambda \\bf{I})^T\\vec{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{\\beta}=(X^TX + \\lambda \\bf{I} )^{-1}X^TY\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daaf003-45df-4059-978d-180d97dd4fd3",
   "metadata": {},
   "source": [
    "The beauty of ridge regression, is that the solution is still closed form and thus \n",
    "always solvable.\n",
    "\n",
    "By pushing feature values towards zero, it also helps to prevent the model from\n",
    "over relying on a small subset of features.\n",
    "\n",
    "As discussed above, ridge regression also has a closed form solution, thus we calculate the $\\beta$ parameters in the `fit()` function as follows:\n",
    "\n",
    "$$\\vec{\\beta}=(X^TX + \\lambda \\bf{I} )^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c83e7d5-a7c3-4baf-a3b0-95dfe15adf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ridge_regression:\n",
    "    def __init__(self, X, y, λ):\n",
    "        self.betas = self.fit(X, y, λ)\n",
    "\n",
    "    def fit(self,X, y, λ):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        I = np.identity(X.shape[1])\n",
    "\n",
    "        betas = np.linalg.pinv(X.T @ X + λ * I) @ X.T @ y\n",
    "        return betas\n",
    "\n",
    "    def predict(self, X):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        return X.T @ self.betas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91408772-78c1-4d1c-b8dc-35e403947d1c",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "LASSO regression uses L1-Normalization. So rather than taking the sum of squares of\n",
    "the coefficients and adding that to the loss the function, we take the sum of absolute\n",
    "values of the coefficients.\n",
    "\n",
    "By doing this, the penalty for coefficient magnitude is linear, versus exponential\n",
    "like it was for ridge regression. This has the effect of not just pushing coefficients\n",
    "to below 1, but all the way to 0. If the coefficient has a value of 0.5, then it deals\n",
    "a penalty of 0.5 with LASSO regression. Whereas with ridge regression, the smaller the coefficient,\n",
    "then the penalty gets exponentially smaller. Ie. a coefficient of 0.5 would have a penalty\n",
    "of $0.5^2 = 0.25$. \n",
    "\n",
    "With that being said, LASSO regression thus works as a method of automatic feature selection,\n",
    "since it will push coefficients towards 0, as well as a regularizaiton method!\n",
    "\n",
    "The downside is that LASSO regression does NOT have a closed form solution, and thus we\n",
    "don't have guarantee that we ever find the optimal solution. Since it doesn't have a closed\n",
    "form solution, an iterative optimization algorithm is typically ran on the loss function\n",
    "to find the values of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62d53b-5ffd-4a98-949a-e2425e6ebf6c",
   "metadata": {},
   "source": [
    "The loss function being minimized is\n",
    "$$\\vec{\\hat{\\beta}}=\n",
    "\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}}\n",
    "\\sum_{i=1}^{n}{(\\hat{\\beta} .\\vec{x_i} - y_i)^2}+\\lambda |\\hat{\\beta}_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39596a89-6d16-4c77-b154-c0249f0bf861",
   "metadata": {},
   "source": [
    "Something to note, if you set $\\lambda$ to 0, our loss function is identical to regular linear regression's loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a756af65-2ce9-41c2-9de1-427d0d1ad155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression:\n",
    "    def __init__(self, X, y, λ):\n",
    "        self.betas = self.fit(X, y, λ)\n",
    "\n",
    "    def fit(self,X, y, λ):\n",
    "        # Add Bias\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "\n",
    "        # Define loss function\n",
    "        loss_function = lambda betas, X, y, λ: np.sum(((X @ betas) - y)**2) +  λ * np.abs(betas).sum()\n",
    "        # Initialize parameters\n",
    "        betas = np.random.normal(0,.001,X.shape[1])\n",
    "        # Minimize loss function by adjusting parameters\n",
    "        res = minimize(loss_function, x0=betas, args=(X, y, λ))\n",
    "        #Select optimized parameters\n",
    "        betas = res.x        \n",
    "        # Return optimized parameters\n",
    "        return betas\n",
    "\n",
    "    def predict(self, X):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        return X.T @ self.betas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6297629-3aaf-4215-8bd0-e1ca686d3f7b",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "\n",
    "Finally we have elastic net regression. It is a combination of both LASSO and ridge regression. It has two hyper parameter terms now $\\lambda_1$ and $\\lambda_2$. $\\lambda_1$ controls extent of L2-regularization, and $\\lambda_2$ controls the extent of L1-regularization. If either of the two parameters are set to 0, then that effectively removes that regularization function. Below is the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499d2a4-0345-47d3-a455-10e7107eb97a",
   "metadata": {},
   "source": [
    "$$\\vec{\\hat{\\beta}}=\n",
    "\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}}\n",
    "\\sum_{i=1}^{n}{(\\hat{\\beta} .\\vec{x_i} - y_i)^2}+\\lambda_1 \\hat{\\beta}_i^2 + \\lambda_2 |\\hat{\\beta}_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d28a05-0692-4e7e-b038-44cde055ef54",
   "metadata": {},
   "source": [
    "Elastic Net regularization is essentially a generalizaiton of ridge and LASSO regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb921f-4b47-43e6-8940-5e457abf5b96",
   "metadata": {},
   "source": [
    "Elasticnet regression does not have a closed form solution either, and thus the `fit()` function looks very similar to lasso regression's. The difference being the loss function. So the `fit()` function is miinimizeing the loss shown above.\n",
    "\n",
    "Note that if you set $\\lambda 1$ to 0, you have lASSO regression, and if you set $\\lambda 2$ to 0, you have ridge regression. If you set both $\\lambda 1$ and $\\lambda 2$ hyperparameters to 0, you get regular linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759ca696-3fc3-46d8-9412-4262edf6748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class elasticnet_regression:\n",
    "    def __init__(self, X, y, λ1, λ2):\n",
    "        self.betas = self.fit(X, y, λ1, λ2)\n",
    "\n",
    "    def fit(self,X, y, λ1, λ2):\n",
    "        # Add Bias\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        # Define loss function\n",
    "        loss_function = lambda betas, X, y, λ1, λ2: np.sum((y - (X @ betas))**2) + λ1 * np.abs(betas).sum() + λ2 * np.square(betas).sum()\n",
    "        # Initialize parameters\n",
    "        betas = np.random.normal(0,.001,X.shape[1])\n",
    "        # Minimize loss function by adjusting parameters\n",
    "        res = minimize(loss_function, x0=betas, args=(X, y,  λ1,  λ2))\n",
    "        #Select optimized parameters\n",
    "        betas = res.x        \n",
    "        # Return optimized parameters\n",
    "        return betas\n",
    "\n",
    "    def predict(self, X):\n",
    "        X=np.append(X, np.ones(X.shape[1]).reshape(1,-1), 0).T\n",
    "        return X.T @ self.betas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab0efb-2dd2-4033-8c25-af2f95d224bd",
   "metadata": {},
   "source": [
    "## Non-Regression Applications\n",
    "\n",
    "Ridge (L2) and LASSO (L1) regularizaiton is used in quite a few other models as well. Neural networks, logistic regression, boosted trees, etc... all have variations that include either L1 or L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af24a0-07e1-413f-aeab-b2a60cb9a00a",
   "metadata": {},
   "source": [
    "For this example, we're going to generate some synthetic data, that should loosely follow $y=1*x+0$, with noise. There are then 10 outliers that are ~100 unites above where they should be.\n",
    "\n",
    "The outliers were added to show how they impact normal linear regression, and how regularizaiton techninques can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3c369-fe53-466c-baef-aa87b745aa00",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28679c6-873e-4157-a40d-a6b02a9af093",
   "metadata": {},
   "source": [
    "In this post we discussed some regularized variants of regular linear regression. Specifically we covered ridge regression, which uses L2 regularization. Lasso regression, which uses L1 regularization, and finally elasticnet, which uses a combination of L1 and L2 regularization. We went over the intuition behind the different methods and implemented them in code. There was no example in this article, but I may add one in a future rendition. It is admittedly difficult to illustrate the need for regularization with a linear model on 2D data :). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
